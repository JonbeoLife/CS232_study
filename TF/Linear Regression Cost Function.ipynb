{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T04:37:46.136077Z",
     "start_time": "2025-01-10T04:37:46.079753Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_train=[1,2,3]\n",
    "y_train=[1,2,3]\n",
    "\n",
    "W=tf.Variable(tf.random.normal([1]), name='weight')\n",
    "b=tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "#Our hypothesis\n",
    "hypothesis=x_train*W+b\n",
    "\n",
    "#cost/loss function\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-y_train))\n",
    "\n",
    "#Minimize\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "#Launch the graph in a session\n",
    "sess=tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'GradientDescentOptimizer'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m cost\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mreduce_mean(tf\u001B[38;5;241m.\u001B[39msquare(hypothesis\u001B[38;5;241m-\u001B[39my_train))\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#Minimize\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m optimizer\u001B[38;5;241m=\u001B[39m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGradientDescentOptimizer\u001B[49m(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m     18\u001B[0m train\u001B[38;5;241m=\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mminimize(cost)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m#Launch the graph in a session\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow._api.v2.train' has no attribute 'GradientDescentOptimizer'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T04:51:35.338275Z",
     "start_time": "2025-01-10T04:51:29.039224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 정의\n",
    "x_train = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "y_train = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# 변수 정의\n",
    "W = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# 학습률 및 옵티마이저 정의\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# 학습 과정\n",
    "for step in range(2001):  # 100번 반복\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 예측값 계산\n",
    "        y_pred = x_train * W + b\n",
    "        # 손실 함수 (MSE)\n",
    "        loss = tf.reduce_mean(tf.square(y_train - y_pred))\n",
    "    \n",
    "    # 그라디언트 계산\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "    # 옵티마이저로 변수 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    \n",
    "    # 10회마다 출력\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy()}, W: {W.numpy()}, b: {b.numpy()}\")\n"
   ],
   "id": "b2dee49465767f37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 16.118175506591797, W: [-1.134555], b: [1.1398058]\n",
      "Step 10, Loss: 1.9098714590072632, W: [-0.1702196], b: [1.5145829]\n",
      "Step 20, Loss: 0.5393380522727966, W: [0.13951638], b: [1.6025763]\n",
      "Step 30, Loss: 0.3920365571975708, W: [0.24696927], b: [1.6027325]\n",
      "Step 40, Loss: 0.36199989914894104, W: [0.29172006], b: [1.5764263]\n",
      "Step 50, Loss: 0.3438814878463745, W: [0.31684673], b: [1.5425812]\n",
      "Step 60, Loss: 0.32761505246162415, W: [0.33564872], b: [1.5070214]\n",
      "Step 70, Loss: 0.3122082054615021, W: [0.35223633], b: [1.4715301]\n",
      "Step 80, Loss: 0.29753467440605164, W: [0.36788413], b: [1.4366429]\n",
      "Step 90, Loss: 0.2835516631603241, W: [0.3829915], b: [1.4025112]\n",
      "Step 100, Loss: 0.27022573351860046, W: [0.3976877], b: [1.3691684]\n",
      "Step 110, Loss: 0.25752606987953186, W: [0.41201842], b: [1.3366114]\n",
      "Step 120, Loss: 0.24542324244976044, W: [0.42600337], b: [1.3048264]\n",
      "Step 130, Loss: 0.23388928174972534, W: [0.43965426], b: [1.2737967]\n",
      "Step 140, Loss: 0.2228972464799881, W: [0.45298], b: [1.2435046]\n",
      "Step 150, Loss: 0.2124219387769699, W: [0.4659888], b: [1.2139328]\n",
      "Step 160, Loss: 0.2024388164281845, W: [0.47868818], b: [1.1850642]\n",
      "Step 170, Loss: 0.19292479753494263, W: [0.4910855], b: [1.1568822]\n",
      "Step 180, Loss: 0.18385814130306244, W: [0.50318795], b: [1.1293706]\n",
      "Step 190, Loss: 0.17521750926971436, W: [0.5150026], b: [1.1025132]\n",
      "Step 200, Loss: 0.16698293387889862, W: [0.5265362], b: [1.0762945]\n",
      "Step 210, Loss: 0.1591353565454483, W: [0.5377956], b: [1.0506992]\n",
      "Step 220, Loss: 0.15165658295154572, W: [0.54878724], b: [1.0257126]\n",
      "Step 230, Loss: 0.14452926814556122, W: [0.5595175], b: [1.0013201]\n",
      "Step 240, Loss: 0.1377369612455368, W: [0.5699926], b: [0.9775079]\n",
      "Step 250, Loss: 0.1312638372182846, W: [0.58021855], b: [0.9542619]\n",
      "Step 260, Loss: 0.12509487569332123, W: [0.5902014], b: [0.93156856]\n",
      "Step 270, Loss: 0.11921591311693192, W: [0.59994686], b: [0.909415]\n",
      "Step 280, Loss: 0.11361316591501236, W: [0.6094605], b: [0.8877883]\n",
      "Step 290, Loss: 0.10827376693487167, W: [0.61874783], b: [0.86667585]\n",
      "Step 300, Loss: 0.10318532586097717, W: [0.62781435], b: [0.8460655]\n",
      "Step 310, Loss: 0.09833598136901855, W: [0.6366653], b: [0.82594526]\n",
      "Step 320, Loss: 0.09371454268693924, W: [0.6453057], b: [0.80630356]\n",
      "Step 330, Loss: 0.08931034803390503, W: [0.6537406], b: [0.787129]\n",
      "Step 340, Loss: 0.08511307835578918, W: [0.661975], b: [0.7684102]\n",
      "Step 350, Loss: 0.08111310750246048, W: [0.6700136], b: [0.75013673]\n",
      "Step 360, Loss: 0.07730105519294739, W: [0.677861], b: [0.7322978]\n",
      "Step 370, Loss: 0.07366817444562912, W: [0.6855217], b: [0.71488297]\n",
      "Step 380, Loss: 0.07020605355501175, W: [0.6930004], b: [0.6978824]\n",
      "Step 390, Loss: 0.06690660119056702, W: [0.7003011], b: [0.6812861]\n",
      "Step 400, Loss: 0.06376227736473083, W: [0.70742816], b: [0.6650846]\n",
      "Step 410, Loss: 0.060765668749809265, W: [0.71438587], b: [0.6492682]\n",
      "Step 420, Loss: 0.05790990591049194, W: [0.72117794], b: [0.63382804]\n",
      "Step 430, Loss: 0.0551883764564991, W: [0.72780854], b: [0.61875504]\n",
      "Step 440, Loss: 0.05259470269083977, W: [0.73428154], b: [0.60404056]\n",
      "Step 450, Loss: 0.05012297257781029, W: [0.7406006], b: [0.58967596]\n",
      "Step 460, Loss: 0.047767359763383865, W: [0.74676937], b: [0.57565296]\n",
      "Step 470, Loss: 0.04552249610424042, W: [0.75279135], b: [0.56196344]\n",
      "Step 480, Loss: 0.04338310286402702, W: [0.7586702], b: [0.54859936]\n",
      "Step 490, Loss: 0.04134425148367882, W: [0.7644093], b: [0.5355531]\n",
      "Step 500, Loss: 0.039401184767484665, W: [0.77001184], b: [0.522817]\n",
      "Step 510, Loss: 0.0375494547188282, W: [0.7754812], b: [0.51038384]\n",
      "Step 520, Loss: 0.03578479215502739, W: [0.7808204], b: [0.49824646]\n",
      "Step 530, Loss: 0.03410305082798004, W: [0.78603274], b: [0.48639774]\n",
      "Step 540, Loss: 0.03250030800700188, W: [0.7911211], b: [0.47483072]\n",
      "Step 550, Loss: 0.030972890555858612, W: [0.7960886], b: [0.46353883]\n",
      "Step 560, Loss: 0.029517294839024544, W: [0.8009378], b: [0.45251545]\n",
      "Step 570, Loss: 0.028130104765295982, W: [0.8056715], b: [0.4417542]\n",
      "Step 580, Loss: 0.026808083057403564, W: [0.8102929], b: [0.43124884]\n",
      "Step 590, Loss: 0.02554820477962494, W: [0.81480426], b: [0.42099336]\n",
      "Step 600, Loss: 0.02434755116701126, W: [0.8192084], b: [0.41098177]\n",
      "Step 610, Loss: 0.02320328913629055, W: [0.8235078], b: [0.40120825]\n",
      "Step 620, Loss: 0.022112807258963585, W: [0.8277049], b: [0.39166716]\n",
      "Step 630, Loss: 0.021073617041110992, W: [0.83180225], b: [0.38235295]\n",
      "Step 640, Loss: 0.020083211362361908, W: [0.8358021], b: [0.3732603]\n",
      "Step 650, Loss: 0.019139399752020836, W: [0.8397069], b: [0.36438382]\n",
      "Step 660, Loss: 0.018239915370941162, W: [0.8435188], b: [0.35571843]\n",
      "Step 670, Loss: 0.017382696270942688, W: [0.8472402], b: [0.34725913]\n",
      "Step 680, Loss: 0.016565775498747826, W: [0.85087293], b: [0.33900094]\n",
      "Step 690, Loss: 0.015787236392498016, W: [0.85441935], b: [0.33093914]\n",
      "Step 700, Loss: 0.015045284293591976, W: [0.8578813], b: [0.32306916]\n",
      "Step 710, Loss: 0.014338207431137562, W: [0.86126107], b: [0.31538627]\n",
      "Step 720, Loss: 0.013664360158145428, W: [0.8645604], b: [0.30788603]\n",
      "Step 730, Loss: 0.013022211380302906, W: [0.8677812], b: [0.3005642]\n",
      "Step 740, Loss: 0.012410198338329792, W: [0.8709256], b: [0.2934165]\n",
      "Step 750, Loss: 0.01182699203491211, W: [0.87399507], b: [0.2864388]\n",
      "Step 760, Loss: 0.011271151714026928, W: [0.87699157], b: [0.27962703]\n",
      "Step 770, Loss: 0.010741443373262882, W: [0.87991685], b: [0.27297726]\n",
      "Step 780, Loss: 0.01023664977401495, W: [0.88277256], b: [0.26648563]\n",
      "Step 790, Loss: 0.009755544364452362, W: [0.88556033], b: [0.26014838]\n",
      "Step 800, Loss: 0.009297081269323826, W: [0.88828176], b: [0.25396183]\n",
      "Step 810, Loss: 0.008860157802700996, W: [0.8909385], b: [0.24792238]\n",
      "Step 820, Loss: 0.00844376441091299, W: [0.8935321], b: [0.24202655]\n",
      "Step 830, Loss: 0.008046939969062805, W: [0.896064], b: [0.23627095]\n",
      "Step 840, Loss: 0.0076687573455274105, W: [0.8985358], b: [0.2306522]\n",
      "Step 850, Loss: 0.0073083615861833096, W: [0.90094864], b: [0.22516708]\n",
      "Step 860, Loss: 0.006964884232729673, W: [0.90330416], b: [0.2198124]\n",
      "Step 870, Loss: 0.00663756625726819, W: [0.9056036], b: [0.21458505]\n",
      "Step 880, Loss: 0.00632562255486846, W: [0.90784854], b: [0.209482]\n",
      "Step 890, Loss: 0.0060283406637609005, W: [0.91004], b: [0.20450029]\n",
      "Step 900, Loss: 0.005745034199208021, W: [0.91217935], b: [0.19963707]\n",
      "Step 910, Loss: 0.005475025158375502, W: [0.9142678], b: [0.19488952]\n",
      "Step 920, Loss: 0.005217717960476875, W: [0.91630656], b: [0.19025485]\n",
      "Step 930, Loss: 0.004972516093403101, W: [0.91829675], b: [0.18573044]\n",
      "Step 940, Loss: 0.004738830961287022, W: [0.92023987], b: [0.18131362]\n",
      "Step 950, Loss: 0.004516118671745062, W: [0.9221366], b: [0.1770018]\n",
      "Step 960, Loss: 0.0043038721196353436, W: [0.9239882], b: [0.17279255]\n",
      "Step 970, Loss: 0.004101613070815802, W: [0.92579585], b: [0.1686834]\n",
      "Step 980, Loss: 0.003908852115273476, W: [0.92756057], b: [0.16467196]\n",
      "Step 990, Loss: 0.0037251494359225035, W: [0.9292833], b: [0.16075589]\n",
      "Step 1000, Loss: 0.003550076624378562, W: [0.93096495], b: [0.15693294]\n",
      "Step 1010, Loss: 0.003383239032700658, W: [0.9326067], b: [0.15320092]\n",
      "Step 1020, Loss: 0.003224240383133292, W: [0.93420935], b: [0.14955764]\n",
      "Step 1030, Loss: 0.003072715364396572, W: [0.9357739], b: [0.14600103]\n",
      "Step 1040, Loss: 0.002928307978436351, W: [0.9373013], b: [0.14252898]\n",
      "Step 1050, Loss: 0.002790684811770916, W: [0.9387923], b: [0.1391395]\n",
      "Step 1060, Loss: 0.002659531543031335, W: [0.9402479], b: [0.13583061]\n",
      "Step 1070, Loss: 0.002534539671614766, W: [0.94166887], b: [0.13260043]\n",
      "Step 1080, Loss: 0.0024154256097972393, W: [0.94305605], b: [0.12944704]\n",
      "Step 1090, Loss: 0.00230191252194345, W: [0.94441026], b: [0.12636864]\n",
      "Step 1100, Loss: 0.002193725435063243, W: [0.94573224], b: [0.12336344]\n",
      "Step 1110, Loss: 0.0020906359422951937, W: [0.94702274], b: [0.12042975]\n",
      "Step 1120, Loss: 0.001992377219721675, W: [0.94828254], b: [0.11756583]\n",
      "Step 1130, Loss: 0.0018987488001585007, W: [0.9495125], b: [0.11477002]\n",
      "Step 1140, Loss: 0.001809514593333006, W: [0.9507131], b: [0.11204068]\n",
      "Step 1150, Loss: 0.0017244763439521194, W: [0.95188516], b: [0.10937627]\n",
      "Step 1160, Loss: 0.0016434245044365525, W: [0.9530294], b: [0.10677519]\n",
      "Step 1170, Loss: 0.001566191203892231, W: [0.9541464], b: [0.10423599]\n",
      "Step 1180, Loss: 0.0014925837749615312, W: [0.95523685], b: [0.10175717]\n",
      "Step 1190, Loss: 0.0014224429614841938, W: [0.9563014], b: [0.09933729]\n",
      "Step 1200, Loss: 0.0013555901823565364, W: [0.9573406], b: [0.09697495]\n",
      "Step 1210, Loss: 0.0012918850407004356, W: [0.9583551], b: [0.09466878]\n",
      "Step 1220, Loss: 0.0012311720056459308, W: [0.95934546], b: [0.09241746]\n",
      "Step 1230, Loss: 0.0011733070714399219, W: [0.96031225], b: [0.09021964]\n",
      "Step 1240, Loss: 0.0011181654408574104, W: [0.961256], b: [0.08807413]\n",
      "Step 1250, Loss: 0.0010656145168468356, W: [0.9621774], b: [0.08597964]\n",
      "Step 1260, Loss: 0.0010155364871025085, W: [0.9630769], b: [0.08393497]\n",
      "Step 1270, Loss: 0.0009678094647824764, W: [0.9639549], b: [0.08193892]\n",
      "Step 1280, Loss: 0.0009223318775184453, W: [0.9648121], b: [0.07999034]\n",
      "Step 1290, Loss: 0.0008789829444140196, W: [0.9656489], b: [0.0780881]\n",
      "Step 1300, Loss: 0.0008376711048185825, W: [0.96646583], b: [0.07623111]\n",
      "Step 1310, Loss: 0.0007983070681802928, W: [0.9672633], b: [0.07441825]\n",
      "Step 1320, Loss: 0.0007607881561852992, W: [0.9680418], b: [0.07264852]\n",
      "Step 1330, Loss: 0.000725031306501478, W: [0.96880174], b: [0.07092087]\n",
      "Step 1340, Loss: 0.0006909610820002854, W: [0.9695436], b: [0.06923434]\n",
      "Step 1350, Loss: 0.0006584877264685929, W: [0.970268], b: [0.0675879]\n",
      "Step 1360, Loss: 0.0006275403429754078, W: [0.970975], b: [0.0659806]\n",
      "Step 1370, Loss: 0.0005980505375191569, W: [0.97166526], b: [0.06441151]\n",
      "Step 1380, Loss: 0.0005699451430700719, W: [0.97233903], b: [0.06287975]\n",
      "Step 1390, Loss: 0.0005431597237475216, W: [0.9729969], b: [0.06138442]\n",
      "Step 1400, Loss: 0.0005176341510377824, W: [0.973639], b: [0.05992465]\n",
      "Step 1410, Loss: 0.0004933053278364241, W: [0.9742659], b: [0.05849959]\n",
      "Step 1420, Loss: 0.0004701220605056733, W: [0.9748779], b: [0.05710844]\n",
      "Step 1430, Loss: 0.00044802771299146116, W: [0.9754753], b: [0.05575035]\n",
      "Step 1440, Loss: 0.0004269727796781808, W: [0.97605854], b: [0.05442455]\n",
      "Step 1450, Loss: 0.00040690519381314516, W: [0.9766279], b: [0.05313028]\n",
      "Step 1460, Loss: 0.00038778423913754523, W: [0.9771837], b: [0.0518668]\n",
      "Step 1470, Loss: 0.0003695605555549264, W: [0.9777263], b: [0.05063337]\n",
      "Step 1480, Loss: 0.0003521913895383477, W: [0.9782559], b: [0.04942927]\n",
      "Step 1490, Loss: 0.0003356402739882469, W: [0.978773], b: [0.0482538]\n",
      "Step 1500, Loss: 0.0003198686463292688, W: [0.97927785], b: [0.04710631]\n",
      "Step 1510, Loss: 0.00030483395676128566, W: [0.9797708], b: [0.04598606]\n",
      "Step 1520, Loss: 0.0002905046276282519, W: [0.9802518], b: [0.04489243]\n",
      "Step 1530, Loss: 0.00027685254462994635, W: [0.9807214], b: [0.04382483]\n",
      "Step 1540, Loss: 0.0002638413570821285, W: [0.98117983], b: [0.04278263]\n",
      "Step 1550, Loss: 0.0002514426887501031, W: [0.9816274], b: [0.04176523]\n",
      "Step 1560, Loss: 0.00023962731938809156, W: [0.9820643], b: [0.04077202]\n",
      "Step 1570, Loss: 0.00022836355492472649, W: [0.98249084], b: [0.03980244]\n",
      "Step 1580, Loss: 0.0002176321140723303, W: [0.98290724], b: [0.0388559]\n",
      "Step 1590, Loss: 0.00020740421314258128, W: [0.98331374], b: [0.03793187]\n",
      "Step 1600, Loss: 0.0001976555649889633, W: [0.9837105], b: [0.0370298]\n",
      "Step 1610, Loss: 0.00018836792150978, W: [0.9840979], b: [0.0361492]\n",
      "Step 1620, Loss: 0.00017951603513211012, W: [0.9844761], b: [0.03528954]\n",
      "Step 1630, Loss: 0.0001710804208414629, W: [0.98484534], b: [0.03445031]\n",
      "Step 1640, Loss: 0.00016303830489050597, W: [0.9852057], b: [0.03363103]\n",
      "Step 1650, Loss: 0.00015537549916189164, W: [0.9855575], b: [0.03283123]\n",
      "Step 1660, Loss: 0.00014807477418798953, W: [0.98590094], b: [0.03205048]\n",
      "Step 1670, Loss: 0.0001411161938449368, W: [0.9862362], b: [0.03128828]\n",
      "Step 1680, Loss: 0.00013448366371449083, W: [0.98656356], b: [0.03054422]\n",
      "Step 1690, Loss: 0.000128163504996337, W: [0.98688304], b: [0.02981784]\n",
      "Step 1700, Loss: 0.00012214029266033322, W: [0.98719496], b: [0.02910876]\n",
      "Step 1710, Loss: 0.00011639964213827625, W: [0.9874995], b: [0.02841654]\n",
      "Step 1720, Loss: 0.00011092953354818746, W: [0.9877968], b: [0.02774078]\n",
      "Step 1730, Loss: 0.00010571581515250728, W: [0.988087], b: [0.02708108]\n",
      "Step 1740, Loss: 0.00010074884630739689, W: [0.98837024], b: [0.02643707]\n",
      "Step 1750, Loss: 9.60127217695117e-05, W: [0.9886468], b: [0.02580838]\n",
      "Step 1760, Loss: 9.150135883828625e-05, W: [0.98891675], b: [0.02519465]\n",
      "Step 1770, Loss: 8.720118785277009e-05, W: [0.9891803], b: [0.02459551]\n",
      "Step 1780, Loss: 8.31030192784965e-05, W: [0.9894375], b: [0.02401065]\n",
      "Step 1790, Loss: 7.919845666037872e-05, W: [0.98968875], b: [0.02343971]\n",
      "Step 1800, Loss: 7.547502900706604e-05, W: [0.989934], b: [0.02288231]\n",
      "Step 1810, Loss: 7.192905468400568e-05, W: [0.99017334], b: [0.02233816]\n",
      "Step 1820, Loss: 6.854911771370098e-05, W: [0.99040717], b: [0.02180694]\n",
      "Step 1830, Loss: 6.532673432957381e-05, W: [0.9906353], b: [0.02128833]\n",
      "Step 1840, Loss: 6.225739343790337e-05, W: [0.99085796], b: [0.02078205]\n",
      "Step 1850, Loss: 5.9330795920686796e-05, W: [0.9910754], b: [0.02028783]\n",
      "Step 1860, Loss: 5.654316919390112e-05, W: [0.9912876], b: [0.01980536]\n",
      "Step 1870, Loss: 5.38843305548653e-05, W: [0.9914948], b: [0.01933436]\n",
      "Step 1880, Loss: 5.1353126764297485e-05, W: [0.9916971], b: [0.01887457]\n",
      "Step 1890, Loss: 4.893998629995622e-05, W: [0.9918945], b: [0.01842571]\n",
      "Step 1900, Loss: 4.663992513087578e-05, W: [0.99208724], b: [0.01798754]\n",
      "Step 1910, Loss: 4.4447690015658736e-05, W: [0.9922754], b: [0.01755978]\n",
      "Step 1920, Loss: 4.2358024074928835e-05, W: [0.9924592], b: [0.01714218]\n",
      "Step 1930, Loss: 4.036728569190018e-05, W: [0.9926385], b: [0.01673451]\n",
      "Step 1940, Loss: 3.8470461731776595e-05, W: [0.9928136], b: [0.01633652]\n",
      "Step 1950, Loss: 3.666327756945975e-05, W: [0.9929845], b: [0.01594801]\n",
      "Step 1960, Loss: 3.4939443139592186e-05, W: [0.9931513], b: [0.01556874]\n",
      "Step 1970, Loss: 3.329691753606312e-05, W: [0.9933142], b: [0.01519848]\n",
      "Step 1980, Loss: 3.173211734974757e-05, W: [0.9934732], b: [0.01483703]\n",
      "Step 1990, Loss: 3.0241426429711282e-05, W: [0.9936284], b: [0.01448419]\n",
      "Step 2000, Loss: 2.8819931685575284e-05, W: [0.9937799], b: [0.01413974]\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
