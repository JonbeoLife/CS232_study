{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T06:14:37.080895Z",
     "start_time": "2025-01-10T06:14:31.576001Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data=[73.,93.,89.,96.,73.]\n",
    "x2_data=[80.,88.,91.,98.,66.]\n",
    "x3_data=[75.,93.,90.,100.,70.]\n",
    "y_data=[152.,185.,180.,196.,142.]\n",
    "\n",
    "x1=tf.placeholder(tf.float32)\n",
    "x2=tf.placeholder(tf.float32)\n",
    "x3=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "\n",
    "w1=tf.Variable(tf.random.normal([1]),name='weight1')\n",
    "w2=tf.Variable(tf.random.normal([1]),name='weight2')\n",
    "w3=tf.Variable(tf.random.normal([1]),name='weight3')\n",
    "b=tf.Variable(tf.random.normal([1]),name='bias')\n",
    "hypothesis=x1*w1+x2*w2+x3*w3+b\n",
    "\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val,_=sess.run([cost,hypothesis,train],\n",
    "                                feed_dict={x1:x1_data,x2:x2_data,x3:x3_data,Y:y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step,\"Cost: \", cost_val,\"\\nPrediction:\\n\", hy_val)\n",
    "        "
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m x3_data\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m75.\u001B[39m,\u001B[38;5;241m93.\u001B[39m,\u001B[38;5;241m90.\u001B[39m,\u001B[38;5;241m100.\u001B[39m,\u001B[38;5;241m70.\u001B[39m]\n\u001B[0;32m      6\u001B[0m y_data\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m152.\u001B[39m,\u001B[38;5;241m185.\u001B[39m,\u001B[38;5;241m180.\u001B[39m,\u001B[38;5;241m196.\u001B[39m,\u001B[38;5;241m142.\u001B[39m]\n\u001B[1;32m----> 8\u001B[0m x1\u001B[38;5;241m=\u001B[39m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplaceholder\u001B[49m(tf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m      9\u001B[0m x2\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mplaceholder(tf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     10\u001B[0m x3\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mplaceholder(tf\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T06:15:51.356405Z",
     "start_time": "2025-01-10T06:15:45.829494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 정의\n",
    "x1_data = tf.constant([73., 93., 89., 96., 73.], dtype=tf.float32)\n",
    "x2_data = tf.constant([80., 88., 91., 98., 66.], dtype=tf.float32)\n",
    "x3_data = tf.constant([75., 93., 90., 100., 70.], dtype=tf.float32)\n",
    "y_data = tf.constant([152., 185., 180., 196., 142.], dtype=tf.float32)\n",
    "\n",
    "# 변수 정의\n",
    "w1 = tf.Variable(tf.random.normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random.normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random.normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# 학습률 정의\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 학습 과정\n",
    "for step in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 가설 정의\n",
    "        hypothesis = x1_data * w1 + x2_data * w2 + x3_data * w3 + b\n",
    "        # 손실 함수 정의\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    \n",
    "    # 그라디언트 계산\n",
    "    gradients = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    # 옵티마이저 적용\n",
    "    w1.assign_sub(learning_rate * gradients[0])\n",
    "    w2.assign_sub(learning_rate * gradients[1])\n",
    "    w3.assign_sub(learning_rate * gradients[2])\n",
    "    b.assign_sub(learning_rate * gradients[3])\n",
    "    \n",
    "    # 10번마다 출력\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step: {step}, Cost: {cost.numpy()}, Prediction: {hypothesis.numpy()}\")\n"
   ],
   "id": "993ae831db0a1413",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Cost: 51162.7109375, Prediction: [-53.812714 -53.43648  -58.32881  -63.33933  -38.4029  ]\n",
      "Step: 10, Cost: 9.917745590209961, Prediction: [146.27353 187.02986 178.61844 194.68883 145.008  ]\n",
      "Step: 20, Cost: 9.399857521057129, Prediction: [146.89072 187.74936 179.33899 195.47205 145.55244]\n",
      "Step: 30, Cost: 9.350543975830078, Prediction: [146.90434 187.74348 179.3448  195.47691 145.54364]\n",
      "Step: 40, Cost: 9.301488876342773, Prediction: [146.91612 187.73544 179.34842 195.47942 145.53319]\n",
      "Step: 50, Cost: 9.252683639526367, Prediction: [146.92786 187.7274  179.35204 195.4819  145.52275]\n",
      "Step: 60, Cost: 9.204146385192871, Prediction: [146.93956 187.7194  179.35565 195.48438 145.51233]\n",
      "Step: 70, Cost: 9.155885696411133, Prediction: [146.95123 187.71143 179.35924 195.48688 145.50195]\n",
      "Step: 80, Cost: 9.107889175415039, Prediction: [146.96286 187.70343 179.36282 195.4893  145.49161]\n",
      "Step: 90, Cost: 9.060168266296387, Prediction: [146.97447 187.69551 179.3664  195.49179 145.4813 ]\n",
      "Step: 100, Cost: 9.012666702270508, Prediction: [146.98605 187.68759 179.36996 195.49425 145.471  ]\n",
      "Step: 110, Cost: 8.965437889099121, Prediction: [146.9976  187.67969 179.37352 195.4967  145.46074]\n",
      "Step: 120, Cost: 8.918448448181152, Prediction: [147.00911 187.67178 179.37703 195.49911 145.45049]\n",
      "Step: 130, Cost: 8.871707916259766, Prediction: [147.0206  187.66393 179.38058 195.50157 145.44028]\n",
      "Step: 140, Cost: 8.825261116027832, Prediction: [147.03204 187.65608 179.3841  195.504   145.4301 ]\n",
      "Step: 150, Cost: 8.779059410095215, Prediction: [147.04347 187.64828 179.38762 195.50641 145.41995]\n",
      "Step: 160, Cost: 8.733068466186523, Prediction: [147.05487 187.64049 179.39113 195.50882 145.40982]\n",
      "Step: 170, Cost: 8.687352180480957, Prediction: [147.06622 187.6327  179.39462 195.51123 145.39972]\n",
      "Step: 180, Cost: 8.641883850097656, Prediction: [147.07755 187.62494 179.39812 195.51361 145.38965]\n",
      "Step: 190, Cost: 8.596659660339355, Prediction: [147.08885 187.61722 179.40158 195.51602 145.37961]\n",
      "Step: 200, Cost: 8.551640510559082, Prediction: [147.10013 187.6095  179.40506 195.5184  145.36958]\n",
      "Step: 210, Cost: 8.506912231445312, Prediction: [147.11136 187.6018  179.40851 195.52078 145.35959]\n",
      "Step: 220, Cost: 8.462408065795898, Prediction: [147.12257 187.59413 179.41197 195.52316 145.34964]\n",
      "Step: 230, Cost: 8.41814136505127, Prediction: [147.13376 187.5865  179.41542 195.52551 145.33969]\n",
      "Step: 240, Cost: 8.374144554138184, Prediction: [147.1449  187.57887 179.41885 195.52788 145.32979]\n",
      "Step: 250, Cost: 8.330327033996582, Prediction: [147.15602 187.57124 179.42227 195.53021 145.3199 ]\n",
      "Step: 260, Cost: 8.28681755065918, Prediction: [147.1671  187.56366 179.42567 195.5326  145.31006]\n",
      "Step: 270, Cost: 8.243500709533691, Prediction: [147.17816 187.55609 179.4291  195.53491 145.30023]\n",
      "Step: 280, Cost: 8.200399398803711, Prediction: [147.1892  187.54854 179.43246 195.53726 145.29042]\n",
      "Step: 290, Cost: 8.157574653625488, Prediction: [147.20018 187.54102 179.43585 195.53958 145.28064]\n",
      "Step: 300, Cost: 8.114940643310547, Prediction: [147.21117 187.53351 179.43924 195.54192 145.2709 ]\n",
      "Step: 310, Cost: 8.07257080078125, Prediction: [147.22209 187.52602 179.4426  195.54419 145.26117]\n",
      "Step: 320, Cost: 8.030410766601562, Prediction: [147.23302 187.51857 179.44595 195.54652 145.25148]\n",
      "Step: 330, Cost: 7.9884843826293945, Prediction: [147.2439  187.51111 179.44931 195.54881 145.24182]\n",
      "Step: 340, Cost: 7.94678258895874, Prediction: [147.25475 187.50368 179.45265 195.5511  145.23218]\n",
      "Step: 350, Cost: 7.9053053855896, Prediction: [147.26556 187.49628 179.45598 195.55339 145.22255]\n",
      "Step: 360, Cost: 7.8640594482421875, Prediction: [147.27635 187.48888 179.4593  195.55566 145.21297]\n",
      "Step: 370, Cost: 7.823023319244385, Prediction: [147.28712 187.48152 179.46262 195.55794 145.2034 ]\n",
      "Step: 380, Cost: 7.782221794128418, Prediction: [147.29785 187.47417 179.46593 195.56021 145.19386]\n",
      "Step: 390, Cost: 7.741647243499756, Prediction: [147.30856 187.46686 179.46922 195.56248 145.18436]\n",
      "Step: 400, Cost: 7.7012529373168945, Prediction: [147.31924 187.45953 179.4725  195.56473 145.17487]\n",
      "Step: 410, Cost: 7.661134243011475, Prediction: [147.32988 187.45226 179.47578 195.56699 145.1654 ]\n",
      "Step: 420, Cost: 7.6211652755737305, Prediction: [147.34052 187.44498 179.47906 195.56923 145.15598]\n",
      "Step: 430, Cost: 7.581475734710693, Prediction: [147.3511  187.43774 179.48233 195.57147 145.14658]\n",
      "Step: 440, Cost: 7.541959285736084, Prediction: [147.36166 187.43051 179.48557 195.5737  145.13718]\n",
      "Step: 450, Cost: 7.502681732177734, Prediction: [147.3722  187.4233  179.4888  195.57591 145.12782]\n",
      "Step: 460, Cost: 7.463602542877197, Prediction: [147.3827  187.4161  179.49205 195.57812 145.1185 ]\n",
      "Step: 470, Cost: 7.424722194671631, Prediction: [147.39319 187.40894 179.49527 195.58035 145.10919]\n",
      "Step: 480, Cost: 7.386058807373047, Prediction: [147.40364 187.40178 179.4985  195.58255 145.09991]\n",
      "Step: 490, Cost: 7.347621917724609, Prediction: [147.41405 187.39465 179.5017  195.58475 145.09064]\n",
      "Step: 500, Cost: 7.309406280517578, Prediction: [147.42444 187.38754 179.5049  195.58693 145.08142]\n",
      "Step: 510, Cost: 7.271356105804443, Prediction: [147.43481 187.38045 179.50809 195.58913 145.07222]\n",
      "Step: 520, Cost: 7.233494758605957, Prediction: [147.44516 187.37337 179.51129 195.59132 145.06303]\n",
      "Step: 530, Cost: 7.195903778076172, Prediction: [147.45546 187.36632 179.51443 195.59348 145.05388]\n",
      "Step: 540, Cost: 7.158471584320068, Prediction: [147.46576 187.35928 179.51762 195.59566 145.04477]\n",
      "Step: 550, Cost: 7.1212568283081055, Prediction: [147.476   187.35226 179.52078 195.59781 145.03564]\n",
      "Step: 560, Cost: 7.084230899810791, Prediction: [147.48622 187.34525 179.52391 195.59998 145.02657]\n",
      "Step: 570, Cost: 7.047434329986572, Prediction: [147.49641 187.33829 179.52705 195.60213 145.0175 ]\n",
      "Step: 580, Cost: 7.010810852050781, Prediction: [147.50659 187.33131 179.5302  195.60425 145.00848]\n",
      "Step: 590, Cost: 6.974401950836182, Prediction: [147.51674 187.32439 179.53331 195.6064  144.99948]\n",
      "Step: 600, Cost: 6.938165187835693, Prediction: [147.52686 187.31746 179.53644 195.60854 144.9905 ]\n",
      "Step: 610, Cost: 6.902153015136719, Prediction: [147.53694 187.31056 179.53954 195.61066 144.98154]\n",
      "Step: 620, Cost: 6.866302490234375, Prediction: [147.54701 187.30367 179.54263 195.61278 144.97261]\n",
      "Step: 630, Cost: 6.830664157867432, Prediction: [147.55704 187.29678 179.54573 195.61488 144.9637 ]\n",
      "Step: 640, Cost: 6.795214653015137, Prediction: [147.56706 187.28996 179.54881 195.617   144.95482]\n",
      "Step: 650, Cost: 6.759934902191162, Prediction: [147.57704 187.28311 179.5519  195.61911 144.94595]\n",
      "Step: 660, Cost: 6.724900722503662, Prediction: [147.58699 187.27632 179.55496 195.6212  144.93712]\n",
      "Step: 670, Cost: 6.690014839172363, Prediction: [147.59691 187.2695  179.558   195.62328 144.9283 ]\n",
      "Step: 680, Cost: 6.655301570892334, Prediction: [147.60683 187.26274 179.56107 195.62538 144.91951]\n",
      "Step: 690, Cost: 6.620809078216553, Prediction: [147.6167  187.25598 179.5641  195.62746 144.91075]\n",
      "Step: 700, Cost: 6.5865159034729, Prediction: [147.62654 187.24925 179.56712 195.6295  144.90201]\n",
      "Step: 710, Cost: 6.5524001121521, Prediction: [147.63637 187.24254 179.57016 195.63159 144.89331]\n",
      "Step: 720, Cost: 6.518450736999512, Prediction: [147.64615 187.23581 179.57317 195.63365 144.88461]\n",
      "Step: 730, Cost: 6.4846930503845215, Prediction: [147.65591 187.22914 179.57617 195.63571 144.87593]\n",
      "Step: 740, Cost: 6.451076507568359, Prediction: [147.66566 187.22246 179.57918 195.63776 144.86728]\n",
      "Step: 750, Cost: 6.417713165283203, Prediction: [147.67538 187.21584 179.58217 195.6398  144.85867]\n",
      "Step: 760, Cost: 6.384469509124756, Prediction: [147.68507 187.20918 179.58516 195.64182 144.85007]\n",
      "Step: 770, Cost: 6.351444244384766, Prediction: [147.69473 187.20258 179.58813 195.64384 144.84149]\n",
      "Step: 780, Cost: 6.31858491897583, Prediction: [147.70438 187.19598 179.5911  195.64587 144.83295]\n",
      "Step: 790, Cost: 6.2858781814575195, Prediction: [147.71399 187.1894  179.59406 195.64792 144.8244 ]\n",
      "Step: 800, Cost: 6.253411769866943, Prediction: [147.72357 187.18286 179.59702 195.64993 144.81592]\n",
      "Step: 810, Cost: 6.221051216125488, Prediction: [147.73314 187.1763  179.59996 195.65193 144.80743]\n",
      "Step: 820, Cost: 6.188903331756592, Prediction: [147.74268 187.16978 179.6029  195.65393 144.79898]\n",
      "Step: 830, Cost: 6.156914234161377, Prediction: [147.75218 187.16327 179.60583 195.65593 144.79054]\n",
      "Step: 840, Cost: 6.125121116638184, Prediction: [147.76166 187.1568  179.60875 195.65793 144.78212]\n",
      "Step: 850, Cost: 6.093481540679932, Prediction: [147.77112 187.15031 179.61166 195.65991 144.77374]\n",
      "Step: 860, Cost: 6.062006950378418, Prediction: [147.78055 187.14386 179.61456 195.6619  144.76537]\n",
      "Step: 870, Cost: 6.030747413635254, Prediction: [147.78995 187.13744 179.61746 195.66386 144.75703]\n",
      "Step: 880, Cost: 5.999576091766357, Prediction: [147.79935 187.131   179.62035 195.66585 144.74872]\n",
      "Step: 890, Cost: 5.968621730804443, Prediction: [147.80869 187.12459 179.62323 195.6678  144.7404 ]\n",
      "Step: 900, Cost: 5.937838554382324, Prediction: [147.81802 187.11823 179.62611 195.66977 144.73213]\n",
      "Step: 910, Cost: 5.907215595245361, Prediction: [147.82733 187.11185 179.62898 195.6717  144.72389]\n",
      "Step: 920, Cost: 5.8767523765563965, Prediction: [147.83661 187.1055  179.63184 195.67366 144.71565]\n",
      "Step: 930, Cost: 5.846446990966797, Prediction: [147.84587 187.09917 179.63469 195.67558 144.70744]\n",
      "Step: 940, Cost: 5.816318511962891, Prediction: [147.8551  187.09285 179.63753 195.67754 144.69926]\n",
      "Step: 950, Cost: 5.786351203918457, Prediction: [147.8643  187.08656 179.64037 195.67946 144.69109]\n",
      "Step: 960, Cost: 5.756541728973389, Prediction: [147.87349 187.08028 179.64319 195.68138 144.68295]\n",
      "Step: 970, Cost: 5.726875305175781, Prediction: [147.88264 187.074   179.64601 195.6833  144.67482]\n",
      "Step: 980, Cost: 5.697432518005371, Prediction: [147.89177 187.06776 179.64883 195.68521 144.66675]\n",
      "Step: 990, Cost: 5.6680803298950195, Prediction: [147.90088 187.06152 179.65163 195.68713 144.65866]\n",
      "Step: 1000, Cost: 5.638890743255615, Prediction: [147.90997 187.05531 179.65443 195.68903 144.6506 ]\n",
      "Step: 1010, Cost: 5.609888076782227, Prediction: [147.91902 187.0491  179.65723 195.69093 144.64258]\n",
      "Step: 1020, Cost: 5.581034183502197, Prediction: [147.92805 187.04292 179.66    195.69283 144.63457]\n",
      "Step: 1030, Cost: 5.552365779876709, Prediction: [147.93706 187.03677 179.66278 195.6947  144.62659]\n",
      "Step: 1040, Cost: 5.523789405822754, Prediction: [147.94606 187.03062 179.66556 195.6966  144.61862]\n",
      "Step: 1050, Cost: 5.495398044586182, Prediction: [147.95502 187.02449 179.66832 195.69847 144.61067]\n",
      "Step: 1060, Cost: 5.467156410217285, Prediction: [147.96396 187.01837 179.67107 195.70032 144.60275]\n",
      "Step: 1070, Cost: 5.43906307220459, Prediction: [147.97287 187.01227 179.67381 195.70221 144.59485]\n",
      "Step: 1080, Cost: 5.411159038543701, Prediction: [147.98175 187.0062  179.67654 195.70406 144.58698]\n",
      "Step: 1090, Cost: 5.383364677429199, Prediction: [147.99062 187.00012 179.67929 195.70593 144.57912]\n",
      "Step: 1100, Cost: 5.355743408203125, Prediction: [147.99947 186.9941  179.68202 195.70778 144.57129]\n",
      "Step: 1110, Cost: 5.328263759613037, Prediction: [148.00827 186.98805 179.68472 195.70961 144.56346]\n",
      "Step: 1120, Cost: 5.300925254821777, Prediction: [148.01707 186.98204 179.68744 195.71146 144.55568]\n",
      "Step: 1130, Cost: 5.2737298011779785, Prediction: [148.02585 186.97603 179.69014 195.71329 144.54791]\n",
      "Step: 1140, Cost: 5.246678829193115, Prediction: [148.03459 186.97005 179.69284 195.71512 144.54015]\n",
      "Step: 1150, Cost: 5.219813346862793, Prediction: [148.0433  186.96408 179.69551 195.71693 144.53242]\n",
      "Step: 1160, Cost: 5.193045139312744, Prediction: [148.05202 186.95813 179.6982  195.71877 144.52472]\n",
      "Step: 1170, Cost: 5.166440010070801, Prediction: [148.0607  186.95221 179.70087 195.72058 144.51703]\n",
      "Step: 1180, Cost: 5.139987468719482, Prediction: [148.06934 186.94626 179.70354 195.72238 144.50937]\n",
      "Step: 1190, Cost: 5.113668918609619, Prediction: [148.07797 186.94037 179.7062  195.72418 144.50172]\n",
      "Step: 1200, Cost: 5.087488174438477, Prediction: [148.08658 186.93448 179.70885 195.72598 144.4941 ]\n",
      "Step: 1210, Cost: 5.0614519119262695, Prediction: [148.09515 186.9286  179.7115  195.72778 144.48648]\n",
      "Step: 1220, Cost: 5.035571098327637, Prediction: [148.10371 186.92274 179.71414 195.72955 144.47891]\n",
      "Step: 1230, Cost: 5.00982141494751, Prediction: [148.11226 186.91692 179.71678 195.73134 144.47136]\n",
      "Step: 1240, Cost: 4.984201908111572, Prediction: [148.12077 186.9111  179.71939 195.73314 144.4638 ]\n",
      "Step: 1250, Cost: 4.958720684051514, Prediction: [148.12926 186.90527 179.72202 195.7349  144.45628]\n",
      "Step: 1260, Cost: 4.93337345123291, Prediction: [148.13773 186.89948 179.72461 195.73665 144.44878]\n",
      "Step: 1270, Cost: 4.908181190490723, Prediction: [148.14618 186.8937  179.72723 195.73842 144.44131]\n",
      "Step: 1280, Cost: 4.883118629455566, Prediction: [148.1546  186.88795 179.72983 195.74019 144.43385]\n",
      "Step: 1290, Cost: 4.858177661895752, Prediction: [148.16301 186.8822  179.73242 195.74196 144.42642]\n",
      "Step: 1300, Cost: 4.833367824554443, Prediction: [148.17139 186.87646 179.73499 195.7437  144.41899]\n",
      "Step: 1310, Cost: 4.808701515197754, Prediction: [148.17975 186.87076 179.73756 195.74542 144.41159]\n",
      "Step: 1320, Cost: 4.784186363220215, Prediction: [148.18806 186.86505 179.74013 195.74716 144.4042 ]\n",
      "Step: 1330, Cost: 4.75978946685791, Prediction: [148.19638 186.85938 179.74269 195.7489  144.39685]\n",
      "Step: 1340, Cost: 4.735535621643066, Prediction: [148.20467 186.8537  179.74525 195.75063 144.38953]\n",
      "Step: 1350, Cost: 4.7114057540893555, Prediction: [148.21294 186.84805 179.74782 195.75237 144.38222]\n",
      "Step: 1360, Cost: 4.687395095825195, Prediction: [148.22118 186.8424  179.75034 195.75407 144.37491]\n",
      "Step: 1370, Cost: 4.663509368896484, Prediction: [148.2294  186.83678 179.75288 195.75578 144.36763]\n",
      "Step: 1380, Cost: 4.639762878417969, Prediction: [148.2376  186.83116 179.7554  195.75749 144.36037]\n",
      "Step: 1390, Cost: 4.616143226623535, Prediction: [148.24579 186.82559 179.75792 195.7592  144.35313]\n",
      "Step: 1400, Cost: 4.592648983001709, Prediction: [148.25394 186.81999 179.76044 195.7609  144.34592]\n",
      "Step: 1410, Cost: 4.569299697875977, Prediction: [148.26205 186.81444 179.76294 195.76259 144.3387 ]\n",
      "Step: 1420, Cost: 4.54606819152832, Prediction: [148.27017 186.8089  179.76544 195.76427 144.33153]\n",
      "Step: 1430, Cost: 4.522948265075684, Prediction: [148.27826 186.80334 179.76793 195.76596 144.32437]\n",
      "Step: 1440, Cost: 4.499973297119141, Prediction: [148.28632 186.79782 179.77042 195.76764 144.31723]\n",
      "Step: 1450, Cost: 4.477089881896973, Prediction: [148.29437 186.79231 179.77289 195.76929 144.3101 ]\n",
      "Step: 1460, Cost: 4.454370498657227, Prediction: [148.3024  186.78685 179.77538 195.77097 144.30301]\n",
      "Step: 1470, Cost: 4.431750297546387, Prediction: [148.3104  186.78137 179.77783 195.77263 144.29591]\n",
      "Step: 1480, Cost: 4.409241676330566, Prediction: [148.31839 186.77591 179.7803  195.7743  144.28886]\n",
      "Step: 1490, Cost: 4.386876583099365, Prediction: [148.32634 186.77046 179.78276 195.77596 144.28181]\n",
      "Step: 1500, Cost: 4.3646111488342285, Prediction: [148.33429 186.76503 179.7852  195.7776  144.2748 ]\n",
      "Step: 1510, Cost: 4.342486381530762, Prediction: [148.34221 186.75963 179.78764 195.77925 144.26779]\n",
      "Step: 1520, Cost: 4.320479393005371, Prediction: [148.3501  186.75423 179.79008 195.7809  144.2608 ]\n",
      "Step: 1530, Cost: 4.2985711097717285, Prediction: [148.35799 186.74884 179.79253 195.78255 144.25385]\n",
      "Step: 1540, Cost: 4.276817321777344, Prediction: [148.36583 186.74347 179.79494 195.78416 144.2469 ]\n",
      "Step: 1550, Cost: 4.255154132843018, Prediction: [148.37366 186.73811 179.79735 195.7858  144.23996]\n",
      "Step: 1560, Cost: 4.23360013961792, Prediction: [148.38147 186.73276 179.79976 195.78741 144.23305]\n",
      "Step: 1570, Cost: 4.212198257446289, Prediction: [148.38925 186.72743 179.80215 195.78903 144.22617]\n",
      "Step: 1580, Cost: 4.190863132476807, Prediction: [148.39703 186.72212 179.80457 195.79065 144.21928]\n",
      "Step: 1590, Cost: 4.169680595397949, Prediction: [148.40477 186.71681 179.80695 195.79227 144.21243]\n",
      "Step: 1600, Cost: 4.148593425750732, Prediction: [148.4125  186.71153 179.80933 195.79385 144.2056 ]\n",
      "Step: 1610, Cost: 4.127632141113281, Prediction: [148.42021 186.70625 179.8117  195.79546 144.19879]\n",
      "Step: 1620, Cost: 4.106773376464844, Prediction: [148.4279  186.701   179.81406 195.79704 144.19199]\n",
      "Step: 1630, Cost: 4.086037635803223, Prediction: [148.43556 186.69576 179.81644 195.79865 144.18521]\n",
      "Step: 1640, Cost: 4.065417289733887, Prediction: [148.4432  186.69054 179.81879 195.80023 144.17845]\n",
      "Step: 1650, Cost: 4.04490852355957, Prediction: [148.45082 186.68532 179.82115 195.80182 144.1717 ]\n",
      "Step: 1660, Cost: 4.024505615234375, Prediction: [148.45844 186.68013 179.82349 195.80339 144.165  ]\n",
      "Step: 1670, Cost: 4.004189491271973, Prediction: [148.46603 186.67494 179.82584 195.80498 144.1583 ]\n",
      "Step: 1680, Cost: 3.9840216636657715, Prediction: [148.47359 186.66977 179.82816 195.80653 144.15161]\n",
      "Step: 1690, Cost: 3.963940143585205, Prediction: [148.48112 186.66461 179.83049 195.80807 144.14493]\n",
      "Step: 1700, Cost: 3.943957567214966, Prediction: [148.48865 186.65945 179.83281 195.80966 144.13828]\n",
      "Step: 1710, Cost: 3.9240994453430176, Prediction: [148.49615 186.65433 179.83513 195.81122 144.13165]\n",
      "Step: 1720, Cost: 3.9043502807617188, Prediction: [148.50363 186.64922 179.83743 195.81274 144.12503]\n",
      "Step: 1730, Cost: 3.884700059890747, Prediction: [148.5111  186.6441  179.83974 195.8143  144.11844]\n",
      "Step: 1740, Cost: 3.865158796310425, Prediction: [148.51854 186.63902 179.84203 195.81584 144.11186]\n",
      "Step: 1750, Cost: 3.845736026763916, Prediction: [148.52596 186.63396 179.84431 195.81738 144.1053 ]\n",
      "Step: 1760, Cost: 3.8264012336730957, Prediction: [148.53336 186.62889 179.8466  195.81891 144.09875]\n",
      "Step: 1770, Cost: 3.8071517944335938, Prediction: [148.54074 186.62383 179.84888 195.82045 144.09222]\n",
      "Step: 1780, Cost: 3.7880358695983887, Prediction: [148.54811 186.61879 179.85115 195.82196 144.08574]\n",
      "Step: 1790, Cost: 3.7690207958221436, Prediction: [148.55545 186.61378 179.85341 195.82349 144.07924]\n",
      "Step: 1800, Cost: 3.750100612640381, Prediction: [148.56276 186.60876 179.85568 195.82501 144.07275]\n",
      "Step: 1810, Cost: 3.7312941551208496, Prediction: [148.57007 186.60378 179.85791 195.8265  144.06631]\n",
      "Step: 1820, Cost: 3.712578535079956, Prediction: [148.57735 186.59879 179.86017 195.828   144.05988]\n",
      "Step: 1830, Cost: 3.693967342376709, Prediction: [148.58461 186.59383 179.8624  195.82951 144.05345]\n",
      "Step: 1840, Cost: 3.6754517555236816, Prediction: [148.59186 186.58887 179.86464 195.83101 144.04706]\n",
      "Step: 1850, Cost: 3.6570305824279785, Prediction: [148.59909 186.58392 179.86687 195.8325  144.04068]\n",
      "Step: 1860, Cost: 3.63873291015625, Prediction: [148.6063  186.57901 179.8691  195.83398 144.03432]\n",
      "Step: 1870, Cost: 3.620526075363159, Prediction: [148.61348 186.57411 179.8713  195.8355  144.02797]\n",
      "Step: 1880, Cost: 3.6023850440979004, Prediction: [148.62065 186.56918 179.87352 195.83698 144.02164]\n",
      "Step: 1890, Cost: 3.5843758583068848, Prediction: [148.62779 186.5643  179.87573 195.83844 144.01532]\n",
      "Step: 1900, Cost: 3.5664706230163574, Prediction: [148.63492 186.55943 179.87793 195.8399  144.00903]\n",
      "Step: 1910, Cost: 3.5486130714416504, Prediction: [148.64203 186.55455 179.88011 195.84137 144.00273]\n",
      "Step: 1920, Cost: 3.5309014320373535, Prediction: [148.64912 186.54971 179.88231 195.84283 143.99649]\n",
      "Step: 1930, Cost: 3.5132675170898438, Prediction: [148.65619 186.54488 179.88449 195.84428 143.99023]\n",
      "Step: 1940, Cost: 3.4957218170166016, Prediction: [148.66324 186.54005 179.88666 195.84576 143.984  ]\n",
      "Step: 1950, Cost: 3.478292465209961, Prediction: [148.67027 186.53525 179.88882 195.8472  143.9778 ]\n",
      "Step: 1960, Cost: 3.4609386920928955, Prediction: [148.67729 186.53046 179.891   195.84865 143.9716 ]\n",
      "Step: 1970, Cost: 3.4436779022216797, Prediction: [148.6843  186.52567 179.89314 195.8501  143.96544]\n",
      "Step: 1980, Cost: 3.4265224933624268, Prediction: [148.69127 186.5209  179.89531 195.85153 143.95927]\n",
      "Step: 1990, Cost: 3.409456729888916, Prediction: [148.69823 186.51614 179.89746 195.85298 143.95314]\n",
      "Step: 2000, Cost: 3.392488956451416, Prediction: [148.70515 186.5114  179.8996  195.8544  143.947  ]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T06:26:45.799642Z",
     "start_time": "2025-01-10T06:26:42.214826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 정의\n",
    "x_data = tf.constant([[73., 80., 75.],\n",
    "                      [93., 88., 93.],\n",
    "                      [89., 91., 90.],\n",
    "                      [96., 98., 100.],\n",
    "                      [73., 66., 70.]], dtype=tf.float32)\n",
    "\n",
    "y_data = tf.constant([[152.],\n",
    "                      [185.],\n",
    "                      [180.],\n",
    "                      [196.],\n",
    "                      [142.]], dtype=tf.float32)\n",
    "\n",
    "# 변수 정의\n",
    "W = tf.Variable(tf.random.normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# 학습률 정의\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 학습 과정\n",
    "for step in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 가설 정의\n",
    "        hypothesis = tf.matmul(x_data, W) + b\n",
    "        # 손실 함수 정의 (Mean Squared Error)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    \n",
    "    # 그라디언트 계산\n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    # 옵티마이저 적용 (수동 업데이트)\n",
    "    W.assign_sub(learning_rate * gradients[0])\n",
    "    b.assign_sub(learning_rate * gradients[1])\n",
    "    \n",
    "    # 10번마다 출력\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step: {step}, Cost: {cost.numpy()}, Prediction: {hypothesis.numpy()}\")\n"
   ],
   "id": "8becabc9c08f77e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Cost: 6693.11865234375, Prediction: [[ 81.48586 ]\n",
      " [ 95.36591 ]\n",
      " [ 95.06829 ]\n",
      " [106.682106]\n",
      " [ 69.419014]]\n",
      "Step: 10, Cost: 13.192843437194824, Prediction: [[153.77516]\n",
      " [182.27345]\n",
      " [180.68883]\n",
      " [199.92097]\n",
      " [135.71268]]\n",
      "Step: 20, Cost: 13.08161735534668, Prediction: [[153.98277]\n",
      " [182.54414]\n",
      " [180.94463]\n",
      " [200.19975]\n",
      " [135.92421]]\n",
      "Step: 30, Cost: 13.031840324401855, Prediction: [[153.97229]\n",
      " [182.55266]\n",
      " [180.94211]\n",
      " [200.19727]\n",
      " [135.93575]]\n",
      "Step: 40, Cost: 12.982287406921387, Prediction: [[153.9612 ]\n",
      " [182.56036]\n",
      " [180.93883]\n",
      " [200.19394]\n",
      " [135.94667]]\n",
      "Step: 50, Cost: 12.932954788208008, Prediction: [[153.95015]\n",
      " [182.56807]\n",
      " [180.9356 ]\n",
      " [200.19061]\n",
      " [135.95758]]\n",
      "Step: 60, Cost: 12.88391399383545, Prediction: [[153.93909]\n",
      " [182.57568]\n",
      " [180.9323 ]\n",
      " [200.18729]\n",
      " [135.96844]]\n",
      "Step: 70, Cost: 12.835169792175293, Prediction: [[153.9281 ]\n",
      " [182.58334]\n",
      " [180.92908]\n",
      " [200.18399]\n",
      " [135.97928]]\n",
      "Step: 80, Cost: 12.786636352539062, Prediction: [[153.91714]\n",
      " [182.59097]\n",
      " [180.92584]\n",
      " [200.1807 ]\n",
      " [135.9901 ]]\n",
      "Step: 90, Cost: 12.738351821899414, Prediction: [[153.90619]\n",
      " [182.59856]\n",
      " [180.92259]\n",
      " [200.17738]\n",
      " [136.00087]]\n",
      "Step: 100, Cost: 12.690306663513184, Prediction: [[153.89528]\n",
      " [182.60614]\n",
      " [180.91939]\n",
      " [200.17409]\n",
      " [136.01163]]\n",
      "Step: 110, Cost: 12.642538070678711, Prediction: [[153.8844 ]\n",
      " [182.6137 ]\n",
      " [180.91618]\n",
      " [200.1708 ]\n",
      " [136.02235]]\n",
      "Step: 120, Cost: 12.595049858093262, Prediction: [[153.87355]\n",
      " [182.62122]\n",
      " [180.91296]\n",
      " [200.16753]\n",
      " [136.03304]]\n",
      "Step: 130, Cost: 12.547773361206055, Prediction: [[153.86275]\n",
      " [182.62875]\n",
      " [180.90979]\n",
      " [200.16428]\n",
      " [136.04372]]\n",
      "Step: 140, Cost: 12.500768661499023, Prediction: [[153.85194]\n",
      " [182.63622]\n",
      " [180.90657]\n",
      " [200.161  ]\n",
      " [136.05434]]\n",
      "Step: 150, Cost: 12.453973770141602, Prediction: [[153.84119]\n",
      " [182.64369]\n",
      " [180.90343]\n",
      " [200.15776]\n",
      " [136.06497]]\n",
      "Step: 160, Cost: 12.407386779785156, Prediction: [[153.83047]\n",
      " [182.65117]\n",
      " [180.90027]\n",
      " [200.1545 ]\n",
      " [136.07556]]\n",
      "Step: 170, Cost: 12.361120223999023, Prediction: [[153.81976]\n",
      " [182.65857]\n",
      " [180.8971 ]\n",
      " [200.15125]\n",
      " [136.0861 ]]\n",
      "Step: 180, Cost: 12.315054893493652, Prediction: [[153.8091 ]\n",
      " [182.66599]\n",
      " [180.89395]\n",
      " [200.14803]\n",
      " [136.09665]]\n",
      "Step: 190, Cost: 12.269220352172852, Prediction: [[153.79848]\n",
      " [182.6734 ]\n",
      " [180.89082]\n",
      " [200.1448 ]\n",
      " [136.10716]]\n",
      "Step: 200, Cost: 12.223641395568848, Prediction: [[153.78784]\n",
      " [182.68074]\n",
      " [180.88768]\n",
      " [200.14157]\n",
      " [136.11763]]\n",
      "Step: 210, Cost: 12.178281784057617, Prediction: [[153.77727]\n",
      " [182.6881 ]\n",
      " [180.88457]\n",
      " [200.13835]\n",
      " [136.12808]]\n",
      "Step: 220, Cost: 12.133174896240234, Prediction: [[153.76674]\n",
      " [182.69543]\n",
      " [180.88148]\n",
      " [200.13518]\n",
      " [136.13853]]\n",
      "Step: 230, Cost: 12.088308334350586, Prediction: [[153.7562 ]\n",
      " [182.70273]\n",
      " [180.87837]\n",
      " [200.13196]\n",
      " [136.14891]]\n",
      "Step: 240, Cost: 12.04365348815918, Prediction: [[153.74573]\n",
      " [182.71004]\n",
      " [180.87527]\n",
      " [200.12877]\n",
      " [136.15929]]\n",
      "Step: 250, Cost: 11.999241828918457, Prediction: [[153.73524]\n",
      " [182.71729]\n",
      " [180.87221]\n",
      " [200.12558]\n",
      " [136.16963]]\n",
      "Step: 260, Cost: 11.955063819885254, Prediction: [[153.72484]\n",
      " [182.72453]\n",
      " [180.86913]\n",
      " [200.12239]\n",
      " [136.17995]]\n",
      "Step: 270, Cost: 11.911096572875977, Prediction: [[153.71445]\n",
      " [182.73178]\n",
      " [180.86607]\n",
      " [200.11925]\n",
      " [136.19026]]\n",
      "Step: 280, Cost: 11.867384910583496, Prediction: [[153.70406]\n",
      " [182.73895]\n",
      " [180.863  ]\n",
      " [200.11604]\n",
      " [136.2005 ]]\n",
      "Step: 290, Cost: 11.823877334594727, Prediction: [[153.69373]\n",
      " [182.74615]\n",
      " [180.85997]\n",
      " [200.1129 ]\n",
      " [136.21075]]\n",
      "Step: 300, Cost: 11.780648231506348, Prediction: [[153.68341]\n",
      " [182.75331]\n",
      " [180.85693]\n",
      " [200.10974]\n",
      " [136.22095]]\n",
      "Step: 310, Cost: 11.737627029418945, Prediction: [[153.67313]\n",
      " [182.76045]\n",
      " [180.8539 ]\n",
      " [200.1066 ]\n",
      " [136.23112]]\n",
      "Step: 320, Cost: 11.694783210754395, Prediction: [[153.66289]\n",
      " [182.7676 ]\n",
      " [180.85089]\n",
      " [200.10347]\n",
      " [136.2413 ]]\n",
      "Step: 330, Cost: 11.652131080627441, Prediction: [[153.65263]\n",
      " [182.77469]\n",
      " [180.84787]\n",
      " [200.10031]\n",
      " [136.25143]]\n",
      "Step: 340, Cost: 11.609779357910156, Prediction: [[153.64244]\n",
      " [182.78178]\n",
      " [180.84486]\n",
      " [200.0972 ]\n",
      " [136.26154]]\n",
      "Step: 350, Cost: 11.567583084106445, Prediction: [[153.63228]\n",
      " [182.78886]\n",
      " [180.84186]\n",
      " [200.09407]\n",
      " [136.27162]]\n",
      "Step: 360, Cost: 11.525639533996582, Prediction: [[153.62212]\n",
      " [182.7959 ]\n",
      " [180.83888]\n",
      " [200.09096]\n",
      " [136.28168]]\n",
      "Step: 370, Cost: 11.483949661254883, Prediction: [[153.61203]\n",
      " [182.80293]\n",
      " [180.8359 ]\n",
      " [200.08786]\n",
      " [136.2917 ]]\n",
      "Step: 380, Cost: 11.442442893981934, Prediction: [[153.60193]\n",
      " [182.80992]\n",
      " [180.83293]\n",
      " [200.08475]\n",
      " [136.3017 ]]\n",
      "Step: 390, Cost: 11.401121139526367, Prediction: [[153.59189]\n",
      " [182.81694]\n",
      " [180.83   ]\n",
      " [200.08167]\n",
      " [136.31169]]\n",
      "Step: 400, Cost: 11.360048294067383, Prediction: [[153.58183]\n",
      " [182.8239 ]\n",
      " [180.82703]\n",
      " [200.07857]\n",
      " [136.32162]]\n",
      "Step: 410, Cost: 11.319170951843262, Prediction: [[153.57185]\n",
      " [182.83086]\n",
      " [180.8241 ]\n",
      " [200.07549]\n",
      " [136.33156]]\n",
      "Step: 420, Cost: 11.27855396270752, Prediction: [[153.56189]\n",
      " [182.83778]\n",
      " [180.82117]\n",
      " [200.07243]\n",
      " [136.34146]]\n",
      "Step: 430, Cost: 11.23810863494873, Prediction: [[153.55194]\n",
      " [182.8447 ]\n",
      " [180.81824]\n",
      " [200.06937]\n",
      " [136.35133]]\n",
      "Step: 440, Cost: 11.197835922241211, Prediction: [[153.54204]\n",
      " [182.85161]\n",
      " [180.81535]\n",
      " [200.06631]\n",
      " [136.3612 ]]\n",
      "Step: 450, Cost: 11.15782356262207, Prediction: [[153.53214]\n",
      " [182.85846]\n",
      " [180.81241]\n",
      " [200.06323]\n",
      " [136.371  ]]\n",
      "Step: 460, Cost: 11.117963790893555, Prediction: [[153.52228]\n",
      " [182.86533]\n",
      " [180.80952]\n",
      " [200.06018]\n",
      " [136.38081]]\n",
      "Step: 470, Cost: 11.078386306762695, Prediction: [[153.51245]\n",
      " [182.87215]\n",
      " [180.80663]\n",
      " [200.05714]\n",
      " [136.39058]]\n",
      "Step: 480, Cost: 11.038969039916992, Prediction: [[153.50266]\n",
      " [182.87897]\n",
      " [180.80374]\n",
      " [200.05411]\n",
      " [136.40033]]\n",
      "Step: 490, Cost: 10.999725341796875, Prediction: [[153.49289]\n",
      " [182.88577]\n",
      " [180.8009 ]\n",
      " [200.05109]\n",
      " [136.41008]]\n",
      "Step: 500, Cost: 10.960710525512695, Prediction: [[153.48312]\n",
      " [182.89255]\n",
      " [180.798  ]\n",
      " [200.04803]\n",
      " [136.41975]]\n",
      "Step: 510, Cost: 10.9219331741333, Prediction: [[153.47342]\n",
      " [182.89934]\n",
      " [180.79518]\n",
      " [200.04506]\n",
      " [136.42944]]\n",
      "Step: 520, Cost: 10.883308410644531, Prediction: [[153.46371]\n",
      " [182.90605]\n",
      " [180.79231]\n",
      " [200.04202]\n",
      " [136.43909]]\n",
      "Step: 530, Cost: 10.844952583312988, Prediction: [[153.45404]\n",
      " [182.91277]\n",
      " [180.78947]\n",
      " [200.03903]\n",
      " [136.4487 ]]\n",
      "Step: 540, Cost: 10.806684494018555, Prediction: [[153.44441]\n",
      " [182.9195 ]\n",
      " [180.78664]\n",
      " [200.03603]\n",
      " [136.45831]]\n",
      "Step: 550, Cost: 10.768699645996094, Prediction: [[153.4348 ]\n",
      " [182.92615]\n",
      " [180.78383]\n",
      " [200.03302]\n",
      " [136.46788]]\n",
      "Step: 560, Cost: 10.730891227722168, Prediction: [[153.4252 ]\n",
      " [182.93282]\n",
      " [180.78099]\n",
      " [200.03003]\n",
      " [136.47742]]\n",
      "Step: 570, Cost: 10.693281173706055, Prediction: [[153.41565]\n",
      " [182.93947]\n",
      " [180.7782 ]\n",
      " [200.02707]\n",
      " [136.48695]]\n",
      "Step: 580, Cost: 10.655813217163086, Prediction: [[153.40613]\n",
      " [182.9461 ]\n",
      " [180.77539]\n",
      " [200.02408]\n",
      " [136.49646]]\n",
      "Step: 590, Cost: 10.618586540222168, Prediction: [[153.3966 ]\n",
      " [182.95271]\n",
      " [180.7726 ]\n",
      " [200.02112]\n",
      " [136.50594]]\n",
      "Step: 600, Cost: 10.58156681060791, Prediction: [[153.38716]\n",
      " [182.95932]\n",
      " [180.76984]\n",
      " [200.01817]\n",
      " [136.5154 ]]\n",
      "Step: 610, Cost: 10.544687271118164, Prediction: [[153.37769]\n",
      " [182.9659 ]\n",
      " [180.76706]\n",
      " [200.01521]\n",
      " [136.52483]]\n",
      "Step: 620, Cost: 10.508020401000977, Prediction: [[153.36827]\n",
      " [182.97244]\n",
      " [180.76428]\n",
      " [200.01227]\n",
      " [136.53424]]\n",
      "Step: 630, Cost: 10.471585273742676, Prediction: [[153.35886]\n",
      " [182.97896]\n",
      " [180.76152]\n",
      " [200.00931]\n",
      " [136.5436 ]]\n",
      "Step: 640, Cost: 10.435315132141113, Prediction: [[153.34952]\n",
      " [182.9855 ]\n",
      " [180.7588 ]\n",
      " [200.0064 ]\n",
      " [136.55296]]\n",
      "Step: 650, Cost: 10.39918327331543, Prediction: [[153.34015]\n",
      " [182.99199]\n",
      " [180.75604]\n",
      " [200.00343]\n",
      " [136.56229]]\n",
      "Step: 660, Cost: 10.36326789855957, Prediction: [[153.33086]\n",
      " [182.9985 ]\n",
      " [180.75331]\n",
      " [200.00053]\n",
      " [136.57161]]\n",
      "Step: 670, Cost: 10.327554702758789, Prediction: [[153.32155]\n",
      " [183.00493]\n",
      " [180.75056]\n",
      " [199.9976 ]\n",
      " [136.58089]]\n",
      "Step: 680, Cost: 10.292017936706543, Prediction: [[153.3123 ]\n",
      " [183.01138]\n",
      " [180.74788]\n",
      " [199.99469]\n",
      " [136.59015]]\n",
      "Step: 690, Cost: 10.25666618347168, Prediction: [[153.30305]\n",
      " [183.01782]\n",
      " [180.74516]\n",
      " [199.99179]\n",
      " [136.59938]]\n",
      "Step: 700, Cost: 10.221487045288086, Prediction: [[153.29384]\n",
      " [183.02423]\n",
      " [180.74245]\n",
      " [199.98888]\n",
      " [136.60858]]\n",
      "Step: 710, Cost: 10.1864595413208, Prediction: [[153.28465]\n",
      " [183.03062]\n",
      " [180.73976]\n",
      " [199.98598]\n",
      " [136.61778]]\n",
      "Step: 720, Cost: 10.151689529418945, Prediction: [[153.27548]\n",
      " [183.037  ]\n",
      " [180.73708]\n",
      " [199.98311]\n",
      " [136.62694]]\n",
      "Step: 730, Cost: 10.117023468017578, Prediction: [[153.26636]\n",
      " [183.04337]\n",
      " [180.7344 ]\n",
      " [199.98022]\n",
      " [136.6361 ]]\n",
      "Step: 740, Cost: 10.082544326782227, Prediction: [[153.25723]\n",
      " [183.0497 ]\n",
      " [180.73174]\n",
      " [199.97733]\n",
      " [136.6452 ]]\n",
      "Step: 750, Cost: 10.048274040222168, Prediction: [[153.24817]\n",
      " [183.05605]\n",
      " [180.72908]\n",
      " [199.97449]\n",
      " [136.65431]]\n",
      "Step: 760, Cost: 10.014188766479492, Prediction: [[153.2391 ]\n",
      " [183.06233]\n",
      " [180.72643]\n",
      " [199.97162]\n",
      " [136.66338]]\n",
      "Step: 770, Cost: 9.980181694030762, Prediction: [[153.23007]\n",
      " [183.06862]\n",
      " [180.72377]\n",
      " [199.96873]\n",
      " [136.67244]]\n",
      "Step: 780, Cost: 9.946467399597168, Prediction: [[153.22107]\n",
      " [183.07489]\n",
      " [180.72115]\n",
      " [199.9659 ]\n",
      " [136.68146]]\n",
      "Step: 790, Cost: 9.912887573242188, Prediction: [[153.21208]\n",
      " [183.08115]\n",
      " [180.7185 ]\n",
      " [199.96306]\n",
      " [136.69046]]\n",
      "Step: 800, Cost: 9.879508018493652, Prediction: [[153.20314]\n",
      " [183.08737]\n",
      " [180.71588]\n",
      " [199.96022]\n",
      " [136.69943]]\n",
      "Step: 810, Cost: 9.846259117126465, Prediction: [[153.19418]\n",
      " [183.09358]\n",
      " [180.71326]\n",
      " [199.95737]\n",
      " [136.70837]]\n",
      "Step: 820, Cost: 9.81312084197998, Prediction: [[153.18529]\n",
      " [183.0998 ]\n",
      " [180.71065]\n",
      " [199.95453]\n",
      " [136.71733]]\n",
      "Step: 830, Cost: 9.78022289276123, Prediction: [[153.1764 ]\n",
      " [183.10597]\n",
      " [180.70805]\n",
      " [199.9517 ]\n",
      " [136.72624]]\n",
      "Step: 840, Cost: 9.747516632080078, Prediction: [[153.16754]\n",
      " [183.11214]\n",
      " [180.70546]\n",
      " [199.9489 ]\n",
      " [136.73512]]\n",
      "Step: 850, Cost: 9.714927673339844, Prediction: [[153.15872]\n",
      " [183.1183 ]\n",
      " [180.70288]\n",
      " [199.94608]\n",
      " [136.74399]]\n",
      "Step: 860, Cost: 9.682519912719727, Prediction: [[153.14992]\n",
      " [183.12442]\n",
      " [180.7003 ]\n",
      " [199.94327]\n",
      " [136.75284]]\n",
      "Step: 870, Cost: 9.650289535522461, Prediction: [[153.14114]\n",
      " [183.13055]\n",
      " [180.69774]\n",
      " [199.94048]\n",
      " [136.76166]]\n",
      "Step: 880, Cost: 9.6182279586792, Prediction: [[153.13237]\n",
      " [183.13663]\n",
      " [180.69518]\n",
      " [199.93767]\n",
      " [136.77045]]\n",
      "Step: 890, Cost: 9.586349487304688, Prediction: [[153.12366]\n",
      " [183.14272]\n",
      " [180.69263]\n",
      " [199.93489]\n",
      " [136.77922]]\n",
      "Step: 900, Cost: 9.554573059082031, Prediction: [[153.11496]\n",
      " [183.14879]\n",
      " [180.69006]\n",
      " [199.9321 ]\n",
      " [136.78798]]\n",
      "Step: 910, Cost: 9.523025512695312, Prediction: [[153.10626]\n",
      " [183.15483]\n",
      " [180.68755]\n",
      " [199.92934]\n",
      " [136.7967 ]]\n",
      "Step: 920, Cost: 9.491569519042969, Prediction: [[153.09761]\n",
      " [183.16086]\n",
      " [180.68501]\n",
      " [199.92654]\n",
      " [136.80542]]\n",
      "Step: 930, Cost: 9.460346221923828, Prediction: [[153.08897]\n",
      " [183.16687]\n",
      " [180.68248]\n",
      " [199.9238 ]\n",
      " [136.8141 ]]\n",
      "Step: 940, Cost: 9.429235458374023, Prediction: [[153.08038]\n",
      " [183.17288]\n",
      " [180.67996]\n",
      " [199.92104]\n",
      " [136.82277]]\n",
      "Step: 950, Cost: 9.398269653320312, Prediction: [[153.07178]\n",
      " [183.17885]\n",
      " [180.67744]\n",
      " [199.91826]\n",
      " [136.8314 ]]\n",
      "Step: 960, Cost: 9.36750316619873, Prediction: [[153.06325]\n",
      " [183.18481]\n",
      " [180.67496]\n",
      " [199.91551]\n",
      " [136.84003]]\n",
      "Step: 970, Cost: 9.336908340454102, Prediction: [[153.05469]\n",
      " [183.19075]\n",
      " [180.67245]\n",
      " [199.91278]\n",
      " [136.84862]]\n",
      "Step: 980, Cost: 9.306413650512695, Prediction: [[153.04619]\n",
      " [183.1967 ]\n",
      " [180.66998]\n",
      " [199.91005]\n",
      " [136.85721]]\n",
      "Step: 990, Cost: 9.276106834411621, Prediction: [[153.03769]\n",
      " [183.20259]\n",
      " [180.6675 ]\n",
      " [199.9073 ]\n",
      " [136.86575]]\n",
      "Step: 1000, Cost: 9.245906829833984, Prediction: [[153.02924]\n",
      " [183.20851]\n",
      " [180.66504]\n",
      " [199.90457]\n",
      " [136.8743 ]]\n",
      "Step: 1010, Cost: 9.215944290161133, Prediction: [[153.02078]\n",
      " [183.21437]\n",
      " [180.66255]\n",
      " [199.90184]\n",
      " [136.88278]]\n",
      "Step: 1020, Cost: 9.186100959777832, Prediction: [[153.01237]\n",
      " [183.22023]\n",
      " [180.6601 ]\n",
      " [199.89914]\n",
      " [136.89128]]\n",
      "Step: 1030, Cost: 9.156380653381348, Prediction: [[153.00397]\n",
      " [183.22609]\n",
      " [180.65764]\n",
      " [199.89641]\n",
      " [136.89973]]\n",
      "Step: 1040, Cost: 9.126851081848145, Prediction: [[152.9956 ]\n",
      " [183.23192]\n",
      " [180.6552 ]\n",
      " [199.89372]\n",
      " [136.90819]]\n",
      "Step: 1050, Cost: 9.09743595123291, Prediction: [[152.98729]\n",
      " [183.23773]\n",
      " [180.65277]\n",
      " [199.89102]\n",
      " [136.91663]]\n",
      "Step: 1060, Cost: 9.068194389343262, Prediction: [[152.97896]\n",
      " [183.24353]\n",
      " [180.65034]\n",
      " [199.88832]\n",
      " [136.92502]]\n",
      "Step: 1070, Cost: 9.039105415344238, Prediction: [[152.97066]\n",
      " [183.24931]\n",
      " [180.64792]\n",
      " [199.88564]\n",
      " [136.9334 ]]\n",
      "Step: 1080, Cost: 9.010146141052246, Prediction: [[152.96239]\n",
      " [183.25508]\n",
      " [180.64551]\n",
      " [199.88295]\n",
      " [136.94176]]\n",
      "Step: 1090, Cost: 8.981308937072754, Prediction: [[152.95413]\n",
      " [183.26083]\n",
      " [180.6431 ]\n",
      " [199.88026]\n",
      " [136.9501 ]]\n",
      "Step: 1100, Cost: 8.952664375305176, Prediction: [[152.94589]\n",
      " [183.26656]\n",
      " [180.64069]\n",
      " [199.87758]\n",
      " [136.9584 ]]\n",
      "Step: 1110, Cost: 8.924169540405273, Prediction: [[152.93768]\n",
      " [183.27226]\n",
      " [180.6383 ]\n",
      " [199.87492]\n",
      " [136.9667 ]]\n",
      "Step: 1120, Cost: 8.895821571350098, Prediction: [[152.92953]\n",
      " [183.27798]\n",
      " [180.63593]\n",
      " [199.87227]\n",
      " [136.97498]]\n",
      "Step: 1130, Cost: 8.867605209350586, Prediction: [[152.92137]\n",
      " [183.28366]\n",
      " [180.63353]\n",
      " [199.8696 ]\n",
      " [136.98322]]\n",
      "Step: 1140, Cost: 8.839498519897461, Prediction: [[152.91325]\n",
      " [183.28935]\n",
      " [180.6312 ]\n",
      " [199.86696]\n",
      " [136.99147]]\n",
      "Step: 1150, Cost: 8.811582565307617, Prediction: [[152.90512]\n",
      " [183.29498]\n",
      " [180.6288 ]\n",
      " [199.8643 ]\n",
      " [136.99966]]\n",
      "Step: 1160, Cost: 8.783777236938477, Prediction: [[152.89706]\n",
      " [183.30064]\n",
      " [180.62646]\n",
      " [199.86168]\n",
      " [137.00787]]\n",
      "Step: 1170, Cost: 8.756123542785645, Prediction: [[152.88896]\n",
      " [183.30626]\n",
      " [180.6241 ]\n",
      " [199.85902]\n",
      " [137.01602]]\n",
      "Step: 1180, Cost: 8.728593826293945, Prediction: [[152.88095]\n",
      " [183.31187]\n",
      " [180.62175]\n",
      " [199.8564 ]\n",
      " [137.02419]]\n",
      "Step: 1190, Cost: 8.701202392578125, Prediction: [[152.87292]\n",
      " [183.31747]\n",
      " [180.61942]\n",
      " [199.85378]\n",
      " [137.03232]]\n",
      "Step: 1200, Cost: 8.673982620239258, Prediction: [[152.86493]\n",
      " [183.32303]\n",
      " [180.61708]\n",
      " [199.85115]\n",
      " [137.04042]]\n",
      "Step: 1210, Cost: 8.64688491821289, Prediction: [[152.85695]\n",
      " [183.3286 ]\n",
      " [180.61476]\n",
      " [199.84854]\n",
      " [137.04851]]\n",
      "Step: 1220, Cost: 8.619943618774414, Prediction: [[152.849  ]\n",
      " [183.33414]\n",
      " [180.61246]\n",
      " [199.84595]\n",
      " [137.05658]]\n",
      "Step: 1230, Cost: 8.593118667602539, Prediction: [[152.84108]\n",
      " [183.33968]\n",
      " [180.61015]\n",
      " [199.84334]\n",
      " [137.06462]]\n",
      "Step: 1240, Cost: 8.56640625, Prediction: [[152.83318]\n",
      " [183.3452 ]\n",
      " [180.60785]\n",
      " [199.84073]\n",
      " [137.07265]]\n",
      "Step: 1250, Cost: 8.539905548095703, Prediction: [[152.82527]\n",
      " [183.35068]\n",
      " [180.60555]\n",
      " [199.83813]\n",
      " [137.08063]]\n",
      "Step: 1260, Cost: 8.513448715209961, Prediction: [[152.81743]\n",
      " [183.35617]\n",
      " [180.60326]\n",
      " [199.83556]\n",
      " [137.08864]]\n",
      "Step: 1270, Cost: 8.487211227416992, Prediction: [[152.80957]\n",
      " [183.36162]\n",
      " [180.60097]\n",
      " [199.83296]\n",
      " [137.09657]]\n",
      "Step: 1280, Cost: 8.461007118225098, Prediction: [[152.80177]\n",
      " [183.3671 ]\n",
      " [180.5987 ]\n",
      " [199.83038]\n",
      " [137.10454]]\n",
      "Step: 1290, Cost: 8.435014724731445, Prediction: [[152.79398]\n",
      " [183.37253]\n",
      " [180.59644]\n",
      " [199.82782]\n",
      " [137.11246]]\n",
      "Step: 1300, Cost: 8.409146308898926, Prediction: [[152.7862 ]\n",
      " [183.37793]\n",
      " [180.59416]\n",
      " [199.82524]\n",
      " [137.12035]]\n",
      "Step: 1310, Cost: 8.38334846496582, Prediction: [[152.77846]\n",
      " [183.38335]\n",
      " [180.59192]\n",
      " [199.82268]\n",
      " [137.12825]]\n",
      "Step: 1320, Cost: 8.357759475708008, Prediction: [[152.77074]\n",
      " [183.38873]\n",
      " [180.58968]\n",
      " [199.82011]\n",
      " [137.1361 ]]\n",
      "Step: 1330, Cost: 8.332232475280762, Prediction: [[152.76303]\n",
      " [183.39412]\n",
      " [180.58743]\n",
      " [199.81757]\n",
      " [137.14395]]\n",
      "Step: 1340, Cost: 8.306883811950684, Prediction: [[152.75534]\n",
      " [183.39948]\n",
      " [180.58519]\n",
      " [199.81502]\n",
      " [137.15176]]\n",
      "Step: 1350, Cost: 8.281614303588867, Prediction: [[152.74765]\n",
      " [183.40482]\n",
      " [180.58295]\n",
      " [199.81245]\n",
      " [137.15956]]\n",
      "Step: 1360, Cost: 8.256490707397461, Prediction: [[152.74004]\n",
      " [183.41016]\n",
      " [180.58075]\n",
      " [199.80992]\n",
      " [137.16736]]\n",
      "Step: 1370, Cost: 8.2315092086792, Prediction: [[152.7324 ]\n",
      " [183.41548]\n",
      " [180.57852]\n",
      " [199.8074 ]\n",
      " [137.17513]]\n",
      "Step: 1380, Cost: 8.206639289855957, Prediction: [[152.72482]\n",
      " [183.42078]\n",
      " [180.57631]\n",
      " [199.80486]\n",
      " [137.18286]]\n",
      "Step: 1390, Cost: 8.181927680969238, Prediction: [[152.71725]\n",
      " [183.42609]\n",
      " [180.57413]\n",
      " [199.80237]\n",
      " [137.1906 ]]\n",
      "Step: 1400, Cost: 8.157339096069336, Prediction: [[152.70967]\n",
      " [183.43132]\n",
      " [180.57191]\n",
      " [199.79982]\n",
      " [137.19827]]\n",
      "Step: 1410, Cost: 8.132820129394531, Prediction: [[152.70216]\n",
      " [183.4366 ]\n",
      " [180.56975]\n",
      " [199.79733]\n",
      " [137.206  ]]\n",
      "Step: 1420, Cost: 8.108453750610352, Prediction: [[152.69464]\n",
      " [183.44183]\n",
      " [180.56754]\n",
      " [199.7948 ]\n",
      " [137.21364]]\n",
      "Step: 1430, Cost: 8.084240913391113, Prediction: [[152.68715]\n",
      " [183.44707]\n",
      " [180.56538]\n",
      " [199.79231]\n",
      " [137.22128]]\n",
      "Step: 1440, Cost: 8.06010913848877, Prediction: [[152.67969]\n",
      " [183.45229]\n",
      " [180.56322]\n",
      " [199.78981]\n",
      " [137.22891]]\n",
      "Step: 1450, Cost: 8.03611946105957, Prediction: [[152.67224]\n",
      " [183.45747]\n",
      " [180.56105]\n",
      " [199.78731]\n",
      " [137.23651]]\n",
      "Step: 1460, Cost: 8.012247085571289, Prediction: [[152.66481]\n",
      " [183.46266]\n",
      " [180.55888]\n",
      " [199.78482]\n",
      " [137.2441 ]]\n",
      "Step: 1470, Cost: 7.988484859466553, Prediction: [[152.65742]\n",
      " [183.46783]\n",
      " [180.55675]\n",
      " [199.78235]\n",
      " [137.25168]]\n",
      "Step: 1480, Cost: 7.964829921722412, Prediction: [[152.65002]\n",
      " [183.47298]\n",
      " [180.5546 ]\n",
      " [199.77986]\n",
      " [137.25923]]\n",
      "Step: 1490, Cost: 7.941319942474365, Prediction: [[152.64267]\n",
      " [183.47813]\n",
      " [180.55246]\n",
      " [199.77739]\n",
      " [137.26675]]\n",
      "Step: 1500, Cost: 7.9179182052612305, Prediction: [[152.63533]\n",
      " [183.48326]\n",
      " [180.55034]\n",
      " [199.77493]\n",
      " [137.27428]]\n",
      "Step: 1510, Cost: 7.894686698913574, Prediction: [[152.628  ]\n",
      " [183.48836]\n",
      " [180.5482 ]\n",
      " [199.77246]\n",
      " [137.28174]]\n",
      "Step: 1520, Cost: 7.871476650238037, Prediction: [[152.62073]\n",
      " [183.49348]\n",
      " [180.54611]\n",
      " [199.77002]\n",
      " [137.28925]]\n",
      "Step: 1530, Cost: 7.848427772521973, Prediction: [[152.61342]\n",
      " [183.49854]\n",
      " [180.54398]\n",
      " [199.76753]\n",
      " [137.29668]]\n",
      "Step: 1540, Cost: 7.825506687164307, Prediction: [[152.60617]\n",
      " [183.50362]\n",
      " [180.54189]\n",
      " [199.7651 ]\n",
      " [137.30412]]\n",
      "Step: 1550, Cost: 7.802680969238281, Prediction: [[152.59894]\n",
      " [183.5087 ]\n",
      " [180.5398 ]\n",
      " [199.76266]\n",
      " [137.31154]]\n",
      "Step: 1560, Cost: 7.780006408691406, Prediction: [[152.59172]\n",
      " [183.51372]\n",
      " [180.53769]\n",
      " [199.76022]\n",
      " [137.31892]]\n",
      "Step: 1570, Cost: 7.757397651672363, Prediction: [[152.58452]\n",
      " [183.51874]\n",
      " [180.53558]\n",
      " [199.75777]\n",
      " [137.3263 ]]\n",
      "Step: 1580, Cost: 7.734886169433594, Prediction: [[152.57736]\n",
      " [183.52376]\n",
      " [180.53354]\n",
      " [199.75534]\n",
      " [137.33368]]\n",
      "Step: 1590, Cost: 7.7125396728515625, Prediction: [[152.57019]\n",
      " [183.52876]\n",
      " [180.53146]\n",
      " [199.75293]\n",
      " [137.34102]]\n",
      "Step: 1600, Cost: 7.6902899742126465, Prediction: [[152.56305]\n",
      " [183.53372]\n",
      " [180.52937]\n",
      " [199.75049]\n",
      " [137.34833]]\n",
      "Step: 1610, Cost: 7.668132781982422, Prediction: [[152.55595]\n",
      " [183.53873]\n",
      " [180.52734]\n",
      " [199.7481 ]\n",
      " [137.35565]]\n",
      "Step: 1620, Cost: 7.6460981369018555, Prediction: [[152.54884]\n",
      " [183.54366]\n",
      " [180.52525]\n",
      " [199.74567]\n",
      " [137.36293]]\n",
      "Step: 1630, Cost: 7.624203681945801, Prediction: [[152.54176]\n",
      " [183.5486 ]\n",
      " [180.52322]\n",
      " [199.74327]\n",
      " [137.3702 ]]\n",
      "Step: 1640, Cost: 7.6024017333984375, Prediction: [[152.53473]\n",
      " [183.55354]\n",
      " [180.5212 ]\n",
      " [199.74088]\n",
      " [137.37744]]\n",
      "Step: 1650, Cost: 7.580655097961426, Prediction: [[152.52768]\n",
      " [183.55844]\n",
      " [180.51913]\n",
      " [199.73845]\n",
      " [137.38467]]\n",
      "Step: 1660, Cost: 7.559068202972412, Prediction: [[152.52068]\n",
      " [183.56335]\n",
      " [180.51712]\n",
      " [199.73607]\n",
      " [137.39189]]\n",
      "Step: 1670, Cost: 7.53759765625, Prediction: [[152.51367]\n",
      " [183.56825]\n",
      " [180.51509]\n",
      " [199.73369]\n",
      " [137.39908]]\n",
      "Step: 1680, Cost: 7.516218662261963, Prediction: [[152.50671]\n",
      " [183.5731 ]\n",
      " [180.51308]\n",
      " [199.73131]\n",
      " [137.40627]]\n",
      "Step: 1690, Cost: 7.494940280914307, Prediction: [[152.49974]\n",
      " [183.57797]\n",
      " [180.51105]\n",
      " [199.72891]\n",
      " [137.4134 ]]\n",
      "Step: 1700, Cost: 7.473785400390625, Prediction: [[152.49283]\n",
      " [183.58282]\n",
      " [180.50906]\n",
      " [199.72656]\n",
      " [137.42056]]\n",
      "Step: 1710, Cost: 7.45269775390625, Prediction: [[152.4859 ]\n",
      " [183.58766]\n",
      " [180.50705]\n",
      " [199.72417]\n",
      " [137.42767]]\n",
      "Step: 1720, Cost: 7.431771278381348, Prediction: [[152.479  ]\n",
      " [183.59245]\n",
      " [180.50505]\n",
      " [199.7218 ]\n",
      " [137.43477]]\n",
      "Step: 1730, Cost: 7.410894870758057, Prediction: [[152.47215]\n",
      " [183.5973 ]\n",
      " [180.50308]\n",
      " [199.71947]\n",
      " [137.44188]]\n",
      "Step: 1740, Cost: 7.390152931213379, Prediction: [[152.46527]\n",
      " [183.60207]\n",
      " [180.50108]\n",
      " [199.71709]\n",
      " [137.44893]]\n",
      "Step: 1750, Cost: 7.369539737701416, Prediction: [[152.45845]\n",
      " [183.60684]\n",
      " [180.49911]\n",
      " [199.71475]\n",
      " [137.45598]]\n",
      "Step: 1760, Cost: 7.348952293395996, Prediction: [[152.45164]\n",
      " [183.61163]\n",
      " [180.49716]\n",
      " [199.7124 ]\n",
      " [137.46303]]\n",
      "Step: 1770, Cost: 7.328526973724365, Prediction: [[152.44484]\n",
      " [183.61636]\n",
      " [180.49518]\n",
      " [199.71005]\n",
      " [137.47003]]\n",
      "Step: 1780, Cost: 7.308178901672363, Prediction: [[152.43806]\n",
      " [183.62111]\n",
      " [180.49323]\n",
      " [199.70772]\n",
      " [137.47704]]\n",
      "Step: 1790, Cost: 7.287951469421387, Prediction: [[152.43132]\n",
      " [183.62584]\n",
      " [180.49129]\n",
      " [199.70538]\n",
      " [137.48401]]\n",
      "Step: 1800, Cost: 7.267814636230469, Prediction: [[152.42458]\n",
      " [183.63054]\n",
      " [180.48932]\n",
      " [199.70305]\n",
      " [137.49097]]\n",
      "Step: 1810, Cost: 7.247767448425293, Prediction: [[152.41785]\n",
      " [183.63522]\n",
      " [180.48737]\n",
      " [199.70071]\n",
      " [137.49791]]\n",
      "Step: 1820, Cost: 7.2278242111206055, Prediction: [[152.41116]\n",
      " [183.63992]\n",
      " [180.48546]\n",
      " [199.69841]\n",
      " [137.50485]]\n",
      "Step: 1830, Cost: 7.207983493804932, Prediction: [[152.40448]\n",
      " [183.64462]\n",
      " [180.48354]\n",
      " [199.6961 ]\n",
      " [137.51176]]\n",
      "Step: 1840, Cost: 7.188238620758057, Prediction: [[152.3978 ]\n",
      " [183.64925]\n",
      " [180.4816 ]\n",
      " [199.69377]\n",
      " [137.51865]]\n",
      "Step: 1850, Cost: 7.168576717376709, Prediction: [[152.39116]\n",
      " [183.65392]\n",
      " [180.47969]\n",
      " [199.69147]\n",
      " [137.52553]]\n",
      "Step: 1860, Cost: 7.149056434631348, Prediction: [[152.38454]\n",
      " [183.65855]\n",
      " [180.47777]\n",
      " [199.68918]\n",
      " [137.53238]]\n",
      "Step: 1870, Cost: 7.129612922668457, Prediction: [[152.37794]\n",
      " [183.66316]\n",
      " [180.47586]\n",
      " [199.68687]\n",
      " [137.53922]]\n",
      "Step: 1880, Cost: 7.110250949859619, Prediction: [[152.37135]\n",
      " [183.66777]\n",
      " [180.47398]\n",
      " [199.68457]\n",
      " [137.54604]]\n",
      "Step: 1890, Cost: 7.091015815734863, Prediction: [[152.36479]\n",
      " [183.67236]\n",
      " [180.47208]\n",
      " [199.6823 ]\n",
      " [137.55284]]\n",
      "Step: 1900, Cost: 7.071840763092041, Prediction: [[152.35825]\n",
      " [183.67696]\n",
      " [180.4702 ]\n",
      " [199.68001]\n",
      " [137.55963]]\n",
      "Step: 1910, Cost: 7.052786350250244, Prediction: [[152.3517 ]\n",
      " [183.6815 ]\n",
      " [180.46829]\n",
      " [199.67772]\n",
      " [137.56639]]\n",
      "Step: 1920, Cost: 7.033815860748291, Prediction: [[152.3452 ]\n",
      " [183.68605]\n",
      " [180.46643]\n",
      " [199.67545]\n",
      " [137.57315]]\n",
      "Step: 1930, Cost: 7.014927864074707, Prediction: [[152.33871]\n",
      " [183.69063]\n",
      " [180.46457]\n",
      " [199.67319]\n",
      " [137.5799 ]]\n",
      "Step: 1940, Cost: 6.996130466461182, Prediction: [[152.33223]\n",
      " [183.69513]\n",
      " [180.46268]\n",
      " [199.6709 ]\n",
      " [137.58661]]\n",
      "Step: 1950, Cost: 6.977444648742676, Prediction: [[152.32579]\n",
      " [183.69966]\n",
      " [180.46083]\n",
      " [199.66866]\n",
      " [137.59332]]\n",
      "Step: 1960, Cost: 6.958795070648193, Prediction: [[152.31935]\n",
      " [183.70418]\n",
      " [180.45898]\n",
      " [199.66638]\n",
      " [137.60002]]\n",
      "Step: 1970, Cost: 6.940334320068359, Prediction: [[152.31293]\n",
      " [183.70866]\n",
      " [180.45714]\n",
      " [199.66414]\n",
      " [137.60667]]\n",
      "Step: 1980, Cost: 6.9219231605529785, Prediction: [[152.30653]\n",
      " [183.71315]\n",
      " [180.45529]\n",
      " [199.6619 ]\n",
      " [137.61333]]\n",
      "Step: 1990, Cost: 6.903589725494385, Prediction: [[152.30014]\n",
      " [183.71759]\n",
      " [180.45345]\n",
      " [199.65962]\n",
      " [137.61995]]\n",
      "Step: 2000, Cost: 6.885353088378906, Prediction: [[152.29381]\n",
      " [183.72208]\n",
      " [180.45163]\n",
      " [199.65741]\n",
      " [137.62659]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Set up feed_dict variables inside the loop.\n",
    "for step in range(2001):\n",
    "   cost_val, hy_val, _ = sess.run(\n",
    "       [cost, hypothesis, train], \n",
    "       feed_dict={X: x_data, Y: y_data})\n",
    "   if step % 10 == 0:\n",
    "       print(step, \"Cost: \", cost_val, \n",
    "                  \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "\n",
    "# Ask my score\n",
    "print(\"Your score will be \", sess.run(hypothesis, \n",
    "           feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \", sess.run(hypothesis, \n",
    "           feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))\n"
   ],
   "id": "34f0fc2a0bff5975"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
